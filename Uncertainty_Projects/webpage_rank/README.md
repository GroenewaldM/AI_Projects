# Webpage Rank


## Summary

This program uses a random surfer model and sampling to rank the importance of different web pages.


## Background

When search engines like Google display search results, they do so by placing more “important” and higher-quality pages higher in the search results than less important pages. But how does the search engine know which pages are more important than other pages?

One heuristic might be that an “important” page is one that many other pages link to, since it’s reasonable to imagine that more sites will link to a higher-quality webpage than a lower-quality webpage. We could therefore imagine a system where each page is given a rank according to the number of incoming links it has from other pages, and higher ranks would signal higher importance.

But this definition isn’t perfect: if someone wants to make their page seem more important, then under this system, they could simply create many other pages that link to their desired page to artificially inflate its rank.

For that reason, the PageRank algorithm was created by Google’s co-founders (including Larry Page, for whom the algorithm was named). In PageRank’s algorithm, a website is more important if it is linked to by other important websites, and links from less important websites have their links weighted less. 

Random Surfer Model: One way to think about PageRank is with the random surfer model, which considers the behavior of a hypothetical surfer on the internet who clicks on links at random. Consider the corpus of web pages below, where an arrow between two pages indicates a link from one page to another.


## Getting Started

* Download the distribution code from https://github.com/GroenewaldM/AI_Projects/tree/main/Uncertainty_Projects/webpage_rank.
* Run pagerank.py


## Understanding

In pagerank.py notice first the definition of two constants at the top of the file: DAMPING represents the damping factor and is initially set to 0.85. SAMPLES represents the number of samples we’ll use to estimate PageRank using the sampling method, initially set to 10,000 samples.

Now, take a look at the main function. It expects a command-line argument, which will be the name of a directory of a corpus of web pages we’d like to compute PageRanks for.

The main function calls the sample_pagerank function, whose purpose is to estimate the PageRank of each page by sampling. The function takes as arguments the corpus of pages generated by crawl function, as well as the damping factor and number of samples to use. Ultimately, sample_pagerank returns a dictionary where the keys are each page name and the values are each page’s estimated PageRank (a number between 0 and 1).


### Example Output

py -m pagerank corpus1
PageRank Results from Sampling (n = 10000)
  bfs.html: 0.1209
  dfs.html: 0.0848
  games.html: 0.2271
  minesweeper.html: 0.1207
  minimax.html: 0.1215
  search.html: 0.2061
  tictactoe.html: 0.1189

## Authors

* Project by HarvardX CS50AI: CS50's Introduction to Artificial Intelligence with   Python
* Project completed by Monique Groenewald
